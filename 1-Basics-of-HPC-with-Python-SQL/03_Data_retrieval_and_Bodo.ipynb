{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2632eef0-7713-4374-970c-9e6f33ee8309",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data retrieval and Bodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229ecc4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Having seen some of Bodo's optimizations, let's look closely at how data retrieval performs—with & without Bodo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9eac0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83508e4e-8326-4aa6-a6aa-7b041b7fcf22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "pd.set_option('display.precision', 2)\n",
    "import time\n",
    "import bodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01fc5e-b30d-47f5-afc0-a90405a6e0ba",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cf728",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ We begin as usual importing useful packages: Pandas, NumPy, `time`, and `bodo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81ea2e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61780547",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def human_bytes(size_bytes):\n",
    "    \"Converts numerical size in Bytes to sensible units\"\n",
    "    assert size_bytes>0, \"Only works with positive values\"\n",
    "    scale = math.floor(math.log(size_bytes)/math.log(1024))\n",
    "    units = ['B', 'kiB', 'MiB', 'GiB', 'TiB']\n",
    "    return f'{size_bytes/(1024**scale):.2f} {units[scale]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d490d1-0826-407d-ac81-811bc14927ce",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993f961",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ We also define a convenient utility function `human_bytes`.\n",
    "+ The math details are less important than what it does:\n",
    "  + convert positive sizes stated in Bytes to more useful units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab84b99",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea78944",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             53 B is 53.00 B\n",
      "          2563 B is 2.50 kiB\n",
      "     35512191 B is 33.87 MiB\n",
      "567123523527 B is 528.17 GiB\n"
     ]
    }
   ],
   "source": [
    "for bsize in [53, 2_563, 35_512_191, 567_123_523_527]:\n",
    "    print(f\"{f'{bsize} B is {human_bytes(bsize)}':>28s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b67319-556f-4e0b-a7c0-a70e052e8bff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb3b2f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ As an example, here we make a list of integers representing file sizes of different scales.\n",
    "+ The function `human_bytes` converts the numbers into strings with Bytes, kilobytes, megabytes, or gigabytes as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90292737",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ecdcc-7a36-4162-b607-bf878ce47b59",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Layout of the data\n",
    "\n",
    "```bash\n",
    "bodo-examples-data/bodo-training-fundamentals/DATA/\n",
    "|\n",
    "└─ CSV ─────────────── PARQUET ────────── PARQUET_010 ────── PARQUET_050\n",
    "```\n",
    "\n",
    "+ Four principle subdirectories of files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bef91a-7233-4c4f-a61b-515fc73a4cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dfb4dd-196b-40ab-9a50-24011983c843",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ The data we're working with is stored in a remote *S3 bucket*.\n",
    "+ It's conceptually laid out with main subdirectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7df173-e42f-4539-9139-875fe56c23bb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a644e",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Layout of the data\n",
    "\n",
    "```bash\n",
    "bodo-examples-data/bodo-training-fundamentals/DATA/\n",
    "|\n",
    "└─ CSV ─────────────── PARQUET ────────── PARQUET_010 ────── PARQUET_050\n",
    "    ├── samples_001.csv  ├── samples_001.pq  ├── samples_001.pq  ├── ...\n",
    "    ├── samples_002.csv  ├── samples_002.pq  ├── samples_002.pq  ├── ...\n",
    "    :                    :                   :                   :\n",
    "    :                    :                   └── samples_010.pq  :\n",
    "    :                    :                                       :\n",
    "    :                    :                                       └── ...\n",
    "    ├── samples_499.csv  ├── samples_499.pq\n",
    "    └── samples_500.csv  └── samples_500.pq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d6286-d62e-48d1-86e0-84bf6a560d43",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd9ba1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ The directories `CSV` & `PARQUET` each hold 500 individual files...\n",
    "  + ...with one million rows of data encoded differently—CSV and Parquet formats respectively.\n",
    "+ The directories `PARQUET_010` & `PARQUET_050` hold smaller subsets of the Parquet data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94373448",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4074dcf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = 'bodo-examples-data/bodo-training-fundamentals/DATA'\n",
    "\n",
    "loading_opts = dict(storage_options=dict(anon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70bf411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "s3 = S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9d770e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129939814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.du(f'{DATA_ROOT}/CSV/samples_001.csv') # gets file size in Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332aa3df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9eb8a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ We'll define `DATA_ROOT` to represent the main path in the public S3 bucket containing our data.\n",
    "+ We'll also define `loading_opts` as a dictionary to use with Pandas functions for reading dataframes.\n",
    "\n",
    "---\n",
    "\n",
    "+ Importing the class `S3FileSysyem` from the module `s3fs` enables exploration of this public S3 bucket.\n",
    "+ To do this, instantiate an object `s3` from this class.\n",
    "\n",
    "---\n",
    "\n",
    "+ The `s3` object now has numerous useful methods mimicking Unix file operations.\n",
    "+ For instance, the `du` method returns the \"disk usage\" of a file or directory as the Unix command would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5b3d4-af1a-4ca8-a043-368ee9d9fe6a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734e5ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_001.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_002.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_003.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_004.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_005.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_006.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_007.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_008.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_009.pq',\n",
       " 'bodo-examples-data/bodo-training-fundamentals/DATA/PARQUET_010/samples_010.pq']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.ls(f'{DATA_ROOT}/PARQUET_010') # gets contents of \"directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd413bd0-28e7-4747-bf3a-16cdf2fe7998",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85a8c0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    " + Similarly, the `ls` method applied to a directory-like path returns...\n",
    "  + ... the contents of the directory as a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a2c53",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a13a12db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 33 ms, total: 171 ms\n",
      "Wall time: 2.32 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tomas Talley</td>\n",
       "      <td>49</td>\n",
       "      <td>1994-12-09 13:50:37</td>\n",
       "      <td>16.30</td>\n",
       "      <td>Health</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paulene Greer</td>\n",
       "      <td>31</td>\n",
       "      <td>2004-06-05 14:43:15</td>\n",
       "      <td>129.76</td>\n",
       "      <td>Sporting-Goods</td>\n",
       "      <td>Good : Any element of a tuple can be accessed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barrett Mccray</td>\n",
       "      <td>69</td>\n",
       "      <td>1990-04-06 18:35:10</td>\n",
       "      <td>85.19</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Fine : I don't even care.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Age        Purchase_Date  Purchase_Amount         Product  \\\n",
       "0    Tomas Talley   49  1994-12-09 13:50:37            16.30          Health   \n",
       "1   Paulene Greer   31  2004-06-05 14:43:15           129.76  Sporting-Goods   \n",
       "2  Barrett Mccray   69  1990-04-06 18:35:10            85.19     Electronics   \n",
       "\n",
       "                                      Product_Review  \n",
       "0                                                NaN  \n",
       "1  Good : Any element of a tuple can be accessed ...  \n",
       "2                          Fine : I don't even care.  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_FILE = 'CSV/samples_001.csv'\n",
    "%time df = pd.read_csv(f's3://{DATA_ROOT}/{CSV_FILE}', nrows=3)\n",
    "df  # Shows entire dataframe,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bab95e-d8f8-4ac9-b4fe-ace5e2d7cef5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaac371",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ We can mimic the Unix `head` command to examine the first few rows of a text file.\n",
    "+ The strategy here is to use Pandas `read_csv` with the `nrows` keyword set to a small value.\n",
    "+ This ensures that a small number of rows are read from the S3 bucket & transferred.\n",
    "+ Notice the transfer time is relatively small reflecting the amount of data parsed.\n",
    "\n",
    "---\n",
    "\n",
    "+ Here, the data retrieved is stored internally as a Pandas dataframe.\n",
    "+ As expected, it has only three rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ad5e0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edee33b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Age,Purchase_Date,Purchase_Amount,Product,Product_Review\n",
      "Tomas Talley,49,1994-12-09 13:50:37,16.3,Health,\n",
      "Paulene Greer,31,2004-06-05 14:43:15,129.76,Sporting-Goods,Good : Any element of a tuple can be accessed in constant time.\n",
      "Barrett Mccray,69,1990-04-06 18:35:10,85.19,Electronics,Fine : I don't even care.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a5c50-2017-4318-9e7f-948d8b5355ba",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Converting this dataframe using the `to_csv` method returns the first few rows of the original text file.\n",
    "+ The option `index=False` prevents the rows being enumerated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba2fdc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33be556a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV/samples_001.csv:\t123.92 MiB\n"
     ]
    }
   ],
   "source": [
    "def get_size(S3_PATH):\n",
    "    size = s3.du(S3_PATH)\n",
    "    return human_bytes(size)\n",
    "\n",
    "print(f'{CSV_FILE}:\\t{get_size(f\"{DATA_ROOT}/{CSV_FILE}\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87f09c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "+ Combining `du` with `human_bytes` from above let's us get...\n",
    " + ... a sense of file & directory sizes from the S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead981c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f307d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CSV/samples_001.csv:\t123.92 MiB\n",
      "PARQUET/samples_001.pq:\t 22.94 MiB\n"
     ]
    }
   ],
   "source": [
    "for FILE in ['CSV/samples_001.csv', 'PARQUET/samples_001.pq']:\n",
    "    print(f'{FILE:>22s}:\\t{get_size(f\"{DATA_ROOT}/{FILE}\"):>10s}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2dd472",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "+ This function `get_size` permits us to compare the relative sizes of files encoded as CSV & Parquet.\n",
    "+ Sure enough, the Parquet data is substantially smaller than the same data encoded as CSV text files (about a factor of five).\n",
    "+ The Apache Parquet format is designed with *compressed column-based storage* which also helps with data parsing, retreival, & computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96403b6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c11839",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV         :\t 55.48 GiB\n",
      "PARQUET     :\t 11.15 GiB\n",
      "PARQUET_010 :\t228.43 MiB\n",
      "PARQUET_050 :\t  1.12 GiB\n"
     ]
    }
   ],
   "source": [
    "for PATH in ['CSV', 'PARQUET', 'PARQUET_010', 'PARQUET_050']:\n",
    "    print(f'{PATH:<12}:\\t{get_size(f\"{DATA_ROOT}/{PATH}\"):>10s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683418e0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "+ Examining the directories shows the relative storage requirements...\n",
    "  + ...of CSV versus Parquet formats.\n",
    "+ Again, the CSV data requires about five times as much disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdedb87d-33f8-47a2-abe3-a935e2d79b5f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21c022-c29f-4930-9137-37e8589d82dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d3eca7-fd1a-453f-9707-451698f48dcd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 744 ms, total: 5.6 s\n",
      "Wall time: 1min 26s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>Zonia Browning</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2017-03-30 14:48:36</td>\n",
       "      <td>803.67</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Fine : Initially composing light-hearted and i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name   Age        Purchase_Date  Purchase_Amount    Product  \\\n",
       "999999  Zonia Browning  40.0  2017-03-30 14:48:36           803.67  Computers   \n",
       "\n",
       "                                           Product_Review  \n",
       "999999  Fine : Initially composing light-hearted and i...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SRC = f's3://{DATA_ROOT}/{CSV_FILE}' # Single CSV file\n",
    "%time df = pd.read_csv(DATA_SRC, **loading_opts)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c42973-7e86-4b46-8310-5597212757a4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "+ Let's look more closely at Pandas functions for reading from S3.\n",
    "+ The `read_csv` function can load a CSV file straight from the S3 bucket.\n",
    "+ The resulting dataframe contains records of purchases...\n",
    "   + ...with columns like `Name`, `Age`, `Purchase_Amount`, `Product`, and so on.\n",
    "+ Notice, when reading from CSV, datatypes are *inferred* unless specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491a2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d0dd3-f1ae-45f4-ad32-45f722e7cc55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Examining the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7102452d-3a8e-4d48-8306-ac42893ef254",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   Name             1000000 non-null  object \n",
      " 1   Age              838868 non-null   float64\n",
      " 2   Purchase_Date    1000000 non-null  object \n",
      " 3   Purchase_Amount  1000000 non-null  float64\n",
      " 4   Product          1000000 non-null  object \n",
      " 5   Product_Review   962920 non-null   object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 45.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcb0d2-3ba0-4fc9-82ea-3284653cf61c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "+ To see the column types explicitly, use the `DataFrame.info` method.\n",
    "  + There are a million rows, some with missing entries.\n",
    "  + The memory footprint is about 46 MiB.\n",
    "  + The columns are mostly of type `object` (well, strings, actually) except two of type `float64` (`Age` & `Purchase_Amount`).\n",
    "  + The `Age` column values are inferred as `float64` due to missing (`NaN`, or not-a-number) entries.\n",
    "+ We'll examine more practically encoded data shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff4cc7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3b70056",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specifying column datatypes for CSV parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a600f11e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_cols = ['Purchase_Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af45a26-4b7d-44f6-86b4-6a009fde63cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_dtypes = {'Name': pd.StringDtype(),\n",
    "              'Age': pd.Int32Dtype(),\n",
    "              'Purchase_Amount':np.float16,\n",
    "              'Product':pd.StringDtype(),\n",
    "              'Product_Review':pd.StringDtype()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9306f9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ae455-ffba-404e-a481-3f030e4922a0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ To specify datatypes explicitly when parsing CSV files, use keyword arguments. This approach will be useful later with Bodo.\n",
    "\n",
    "---\n",
    "\n",
    "+ The singleton list `date_cols` specifies that the `Purchase_Date` column should be parsed as `datetime64` data.\n",
    "\n",
    "---\n",
    "\n",
    "+ The dictionary `col_dtypes` associates column names with NumPy or Pandas data types.\n",
    "+ We choose Pandas `Int32Dtype` for the `Age` column making an extension array that permits an integer version of `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d4d66-9b46-4571-91fc-2b2051c622e3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41a10024-a84b-4859-af54-66b11bb20b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "csv_opts = {'parse_dates':date_cols, 'dtype':col_dtypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3385cfbc-04da-490c-8900-7c317b7c4a07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 586 ms, total: 5.3 s\n",
      "Wall time: 40.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>Zonia Browning</td>\n",
       "      <td>40</td>\n",
       "      <td>2017-03-30 14:48:36</td>\n",
       "      <td>803.5</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Fine : Initially composing light-hearted and i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name  Age       Purchase_Date  Purchase_Amount    Product  \\\n",
       "999999  Zonia Browning   40 2017-03-30 14:48:36            803.5  Computers   \n",
       "\n",
       "                                           Product_Review  \n",
       "999999  Fine : Initially composing light-hearted and i...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df = pd.read_csv(DATA_SRC, **csv_opts, **loading_opts)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d54b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb554b4-837f-4826-863c-c66e9f595c28",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ The preceding objects are put together in a dictionary `csv_opts` with keys `parse_dates` & `dtype`.\n",
    "+ These keys are *keyword arguments* for the Pandas `read_csv` function\n",
    "+ The key `dtype` links to the dictionary `col_dtypes` of datatypes for the columns.\n",
    "---\n",
    "+ The subsequent call to `read_csv` is a little cleaner using the dictionary-unpacking operator `**` for keyword arguments.\n",
    "+ Observe parsing the file from S3 is slightly faster due to some of the explicit type-conversions.\n",
    "  + These likely result in more efficient data transfers across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861225f-2276-4a64-8371-3674544c1361",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71faa49d-7e78-40c9-94f5-b743bb82fc0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count    Dtype         \n",
      "---  ------           --------------    -----         \n",
      " 0   Name             1000000 non-null  string        \n",
      " 1   Age              838868 non-null   Int32         \n",
      " 2   Purchase_Date    1000000 non-null  datetime64[ns]\n",
      " 3   Purchase_Amount  1000000 non-null  float16       \n",
      " 4   Product          1000000 non-null  string        \n",
      " 5   Product_Review   962920 non-null   string        \n",
      "dtypes: Int32(1), datetime64[ns](1), float16(1), string(3)\n",
      "memory usage: 37.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939055-73ff-4a3c-928d-8e011f55a0c0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Looking at the schema for this new DataFrame, the column data types have been assigned explicitly as the CSV file is parsed.\n",
    "   + The columns are encoded in memory as strings, 16-bit floats, 32-bit integers, & 64-bit datetimes\n",
    "+ This reduces the memory foot-print & enables efficiently-stored `Null` entries in integer & string columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67021609-8989-4d9b-bc24-8084553685f7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c654a-ef81-4493-9399-fd4152fee45b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "449f961b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARQUET_FILE = 'PARQUET/samples_001.pq' # Single Parquet file\n",
    "DATA_SRC = f's3://{DATA_ROOT}/{PARQUET_FILE}' \n",
    "pq_opts = {'use_nullable_dtypes':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b99c18-218c-4b52-a218-d39dd9b9c2c6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Going beyond CSV storage, Apache Parquet has 3 significant goals:\n",
    "  + Interoperability\n",
    "  + Space efficiency\n",
    "  + Query efficiency\n",
    "---\n",
    "+ Pandas can parse Parquet files.\n",
    "+ We'll use the dictionary `pq_opts` to specify the keyword argument `use_nullable_dtypes`.\n",
    "  + This is not strictly necessary but it can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9de1a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4f98a6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 270 ms, total: 1.36 s\n",
      "Wall time: 19.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>Zonia Browning</td>\n",
       "      <td>40</td>\n",
       "      <td>2017-03-30 14:48:36</td>\n",
       "      <td>803.67</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Fine : Initially composing light-hearted and i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name  Age       Purchase_Date  Purchase_Amount    Product  \\\n",
       "999999  Zonia Browning   40 2017-03-30 14:48:36           803.67  Computers   \n",
       "\n",
       "                                           Product_Review  \n",
       "999999  Fine : Initially composing light-hearted and i...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df = pd.read_parquet(DATA_SRC, **pq_opts, **loading_opts)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf79bec-9ddc-408c-a005-71562578f3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ The function `read_parquet` parses this file in less than half a minute.\n",
    "+ Again, we can examine the data quickly by looking at the last row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa71d62-64b8-4658-b43c-49603f028abc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c349484",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count    Dtype         \n",
      "---  ------           --------------    -----         \n",
      " 0   Name             1000000 non-null  string        \n",
      " 1   Age              838868 non-null   Int32         \n",
      " 2   Purchase_Date    1000000 non-null  datetime64[ns]\n",
      " 3   Purchase_Amount  1000000 non-null  float64       \n",
      " 4   Product          1000000 non-null  string        \n",
      " 5   Product_Review   962920 non-null   string        \n",
      "dtypes: Int32(1), datetime64[ns](1), float64(1), string(3)\n",
      "memory usage: 42.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdeea88-69f6-4331-95b5-9f7e96d44884",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Moreover, the `info` method tells us how the data is processed on parsing from S3.\n",
    "+ The Parquet data is stored column-wise in a compressed binary format—with types encoded.\n",
    "+ This permits the data on disk to preserve the schema efficiently for retrieval and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6237b8-0824-4d3c-90a7-636cd1984d57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef4a39-44c9-488b-a980-918baade6af4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Reading from a directory of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bc3735c-509e-4ecc-9ae7-5a8bc7738642",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.81 s, sys: 2.93 s, total: 12.7 s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Purchase_Date</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>Hsiu Shelton</td>\n",
       "      <td>24</td>\n",
       "      <td>1992-10-27 14:03:09</td>\n",
       "      <td>40.43</td>\n",
       "      <td>Toys</td>\n",
       "      <td>Terrible : Ports are created with the built-in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  Age       Purchase_Date  Purchase_Amount Product  \\\n",
       "9999999  Hsiu Shelton   24 1992-10-27 14:03:09            40.43    Toys   \n",
       "\n",
       "                                            Product_Review  \n",
       "9999999  Terrible : Ports are created with the built-in...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DATA_SRC = f's3://{DATA_ROOT}/PARQUET_010' # Directory\n",
    "df = pd.read_parquet(DATA_SRC, **pq_opts, **loading_opts)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54b52b-c379-4ad0-b891-14bc2c118081",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Pandas's `read_parquet` function supports reading an entire directory of files into a single dataframe\n",
    "+ The leading argument needs to be the folder containing a set of Parquet files encoded with the same schema.\n",
    "+ Using `df.tail()` confirms that the dataframe loaded has 10 million rows.\n",
    "+ Reading CSV files into a single dataframe requires a loop with the `concat` function as seen previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf311e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd8fb5-a7f0-4180-bd6d-15c0ad7dc236",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Trying again with Bodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcdccd-bdba-4931-935b-7f5f60ff0eb4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "+ Load 50 files from `PARQUET_050` \n",
    "  + `df`: dataframe (50,000,000 rows).\n",
    "+ Compute the `df.Purchase_Amount.mean()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d7e0e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Having experimented with using various ways of loading Pandas DataFrames, let's use Bodo again.\n",
    "+ We'll load 50 files from Parquet data each with a million rows.\n",
    "+ Once we've assembled our DataFrame, we'll compute the mean of the `Purchase_Amount` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e595ae7-0523-47a5-a2de-f8496dceff5b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526de7be-0a97-4dd6-911c-93b1e91d60d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "# Timing data retrieval only...\n",
    "@bodo.jit\n",
    "def compute_mean_purchase():\n",
    "    DATA_ROOT = 'bodo-examples-data/bodo-training-fundamentals/DATA'\n",
    "    DATA_SRC = f's3://{DATA_ROOT}/PARQUET_050'\n",
    "    ‎\n",
    "    df = pd.read_parquet(DATA_SRC)\n",
    "    ‎\n",
    "    avg = df['Purchase_Amount'].mean()\n",
    "    return avg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73120bc6-5834-471a-9025-107155997059",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ Again, embed the computation within a function.\n",
    "+ The key parts are:\n",
    "  + retrieving the data with `read_parquet`;\n",
    "  + computing the mean;\n",
    "  + and, of course, the `bodo.jit` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a9767-df0d-4417-80ef-d3ed749875fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee99616-9bae-4e6d-bb6f-dcd9b1b5d013",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "# Timing data retrieval only...\n",
    "@bodo.jit\n",
    "def compute_mean_purchase():\n",
    "    DATA_ROOT = 'bodo-examples-data/bodo-training-fundamentals/DATA'\n",
    "    DATA_SRC = f's3://{DATA_ROOT}/PARQUET_050'\n",
    "    start = time.time()\n",
    "    df = pd.read_parquet(DATA_SRC)\n",
    "    elapsed = time.time() - start\n",
    "    avg = df['Purchase_Amount'].mean()\n",
    "    return avg, elapsed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fb510-a111-40b5-a849-43d1acf0acd6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ This time, the calls to `time.time` are sandwiched around the call to `read_parquet`.\n",
    "+ In addition, rather than printing the elapsed time within the function, it is returned with the function value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0867c-01e8-4f95-b752-50eefcb60446",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d738e86-485c-4968-aca0-b1fcc640564a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Timing data retrieval only...\n",
    "@bodo.jit\n",
    "def compute_mean_purchase():\n",
    "    DATA_ROOT = 'bodo-examples-data/bodo-training-fundamentals/DATA'\n",
    "    DATA_SRC = f's3://{DATA_ROOT}/PARQUET_050'\n",
    "    start = time.time()\n",
    "    df = pd.read_parquet(DATA_SRC)\n",
    "    elapsed = time.time() - start\n",
    "    avg = df['Purchase_Amount'].mean()\n",
    "    return avg, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de81f402-15e8-4927-a503-a62522d7df8d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Purchase_Amount: $188.28\n",
      "Elapsed time: 88.100 s\n"
     ]
    }
   ],
   "source": [
    "avg, elapsed = compute_mean_purchase()\n",
    "print(f'Average Purchase_Amount: ${avg:,.2f}')\n",
    "print(f'Elapsed time: {elapsed:.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c60054-4c49-4c47-be2f-e5433805b950",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ When executed on my laptop, it takes almost a minute & a half just to read the data from S3.\n",
    "+ This doesn't even work without Bodo for this machine.\n",
    " + The Bodo compiler recognizes that only a single column needs to be fetched from S3.\n",
    " + Without the compiler, the entire dataframe would be built up & downloaded from S3.\n",
    " + This crashes the Jupyter kernel on my laptop without care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a42817",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11fc20-72ce-4f3e-bedf-6a0dc6fcdf88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Summary\n",
    "\n",
    "+ Plausible data retrieval bottlenecks:\n",
    "  + Single file vs. many files (API mostly)\n",
    "  + Parquet vs. CSV (or others)\n",
    "  + Local vs. remote storage (e.g., S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2460a9-3e95-49e0-b986-40e6ea9ff713",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5639f-9e7a-4034-9be3-0d7b3dbf1d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "+ When looking at bottlenecks in data retrieval for a computation, consider:\n",
    "  + whether the data is in a single file or in many files (this is may affect the coding APIs available); and\n",
    "  + whether or not the data is stored in an optimized format such as Parquet; and\n",
    "+ Generically, reading Parquet from S3 is a lot faster than reading CSV from an S3 bucket.\n",
    "+ A number of issues are at play:\n",
    "  + moving data across a network;\n",
    "  + smaller file sizes; and\n",
    "  + parsing data from storage into abstract data types in memory.\n",
    "+ We'll discuss all this next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f0f9-55dc-402f-9679-0ad862c356ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "0e9bccc7bf60bab6269ec44b578803b028c16a2bef3331190c88d9fff5017f0e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Bodo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "rise": {
   "auto_select": "none",
   "slideNumber": false,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
